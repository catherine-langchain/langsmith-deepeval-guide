{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0579d627",
   "metadata": {},
   "source": [
    "## Step by Step Guide to Running LangSmith Experiment with DeepEval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaa7e1a-8b23-4389-80f4-f1f4932f8f81",
   "metadata": {},
   "source": [
    "### Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6655ed2-aa2c-4d5f-8b27-f4189a75ac81",
   "metadata": {},
   "source": [
    "Create a LangSmith API Key\n",
    "\n",
    "To create an API key, head to the Settings page. Then click Create API Key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6364e8b5-0485-489a-9b28-3a286b957c67",
   "metadata": {},
   "source": [
    "Loading Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa2a11f-b465-4466-a418-b34446cf6e38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b02499-38d6-4fe6-9065-6869b7e9f528",
   "metadata": {},
   "source": [
    "### Running a LangSmith experiment\n",
    "\n",
    "Running a LangSmith experiment takes four main steps \n",
    "1. Creating dataset (which can be done both in the UI or SDK)\n",
    "2. Defining application logic\n",
    "3. Defining evaluator logic (in this case, we will be using deepeval)\n",
    "4. Running experiment & viewing results\n",
    "\n",
    "In this guide, we will be running evaluations asynchronously via the SDK using aevaluate(), which accepts all of the same arguments as evaluate() but expects the application function to be asynchronous. You can learn more about how to use the evaluate() function [here](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2da026-adb5-4120-b793-86fa9f8736d4",
   "metadata": {},
   "source": [
    "![Evaluation](images/evals-conceptual.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5871165a-de9a-4b99-ad33-952ca5ef3aaf",
   "metadata": {},
   "source": [
    "### Step 1. Creating a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1fb3995-7dba-4534-a66f-1356493a89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "from email_assistant.eval.email_dataset import examples_triage\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client()\n",
    "\n",
    "# Dataset name & description \n",
    "dataset_name = \"Sample deepeval dataset\"\n",
    "description = \"A sample dataset in LangSmith.\"\n",
    "\n",
    "# Create examples \n",
    "examples = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Which country is Mount Kilimanjaro located in?\"},\n",
    "        \"outputs\": {\"answer\": \"Mount Kilimanjaro is located in Tanzania.\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is Earth's lowest point?\"},\n",
    "        \"outputs\": {\"answer\": \"Earth's lowest point is The Dead Sea.\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Who are the winners of 2025 Wimbledon Championship for Singles?\"},\n",
    "        \"outputs\": {\"answer\": \"Iga Swiatek is the winner for Women's Singles, and Jannik Sinner won Men's Singles.\"},\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# Add examples to dataset if it doesn't exist\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name, \n",
    "        description=description\n",
    "    )\n",
    "    # Add examples to the dataset\n",
    "    client.create_examples(dataset_id=dataset.id, examples=examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f472b5f4-f045-4d9e-9d5e-3c463b92a0f5",
   "metadata": {},
   "source": [
    "### Step 2. Defining the application logic that you are evaluating \n",
    "\n",
    "Now, define [target function](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target) that contains what you're evaluating. For example, this may be one LLM call that includes the new prompt you are testing, a part of your application or your end to end application. \n",
    "\n",
    "In our example, we will be testing a simple prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7741a2d7-f881-4555-827d-153883c31dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\")\n",
    "prompt = \"You are a knowledgeable assistant that answers factual questions clearly and concisely. For each question, provide a direct, accurate answer in one sentence. Do not add extra commentary or explanation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f510824c-961c-45c5-bd64-8a96f258293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def target(inputs: dict) -> dict:\n",
    "    response = await llm.ainvoke(\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": inputs[\"question\"]},\n",
    "        ],\n",
    "    )\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3734ba2-ae07-4bdc-bdeb-972975e7dbe5",
   "metadata": {},
   "source": [
    "### Step 3. Define evaluator\n",
    "\n",
    "This could include any custom evaluator logic. In this example, we will be importing an evaluator from the [deepeval](https://github.com/confident-ai/deepeval) package. \n",
    "Outputs are the result of your target function. reference_outputs / referenceOutputs are from the example pairs you defined in step 4 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4dcf2017-e1be-47e6-8687-25e5342488c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U -q deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "36c55d3e-342e-4e8e-a2ff-3fe6287cef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "\n",
    "async def correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n",
    "    correctness_metric = GEval(\n",
    "        name=\"Correctness\", \n",
    "        criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\", \n",
    "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    "    )\n",
    "\n",
    "    test_case = LLMTestCase(\n",
    "        input = inputs[\"question\"],\n",
    "        # Replace this with the actual output from your LLM application\n",
    "        actual_output= outputs[\"answer\"],\n",
    "        expected_output= reference_outputs[\"answer\"]\n",
    "    )\n",
    "    \n",
    "    correctness_metric.measure(test_case)\n",
    "    return {\"score\": correctness_metric.score, \"comment\": correctness_metric.reason}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30929e26-4492-4860-839e-747ce49c7acd",
   "metadata": {},
   "source": [
    "### Step 4. Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb53bd4-6e45-4915-bee4-ea96a947b0f4",
   "metadata": {},
   "source": [
    "After running the evaluation, a link will be provided to view the results in langsmith. An example view of dataset & experiments can be found [here](https://smith.langchain.com/public/8e75cac2-7669-4b4f-b19b-5c7884534a22/d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bf6cf28c-b7e1-4d4d-82aa-f4b9db157ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'first-eval-in-langsmith-321df1ff' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/500bda05-b591-42ac-bf37-6e7a2aeeddc9/compare?selectedSessions=310283d6-9624-4e11-a963-9a485b654d70\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a78e0ff712c466fa13cf7ba3d3a8b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0c9fa1e515441080b1258bcf9f9cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b3ad0def864729899828157f8c4aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cd994a4c804c639d82894ef9c97bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_results = await client.aevaluate(\n",
    "    target,\n",
    "    data=\"Sample deepeval dataset\",\n",
    "    evaluators=[\n",
    "        correctness_evaluator,\n",
    "        # can add multiple evaluators here\n",
    "    ],\n",
    "    experiment_prefix=\"first-eval-in-langsmith\",\n",
    "    max_concurrency=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ffd82a-7a88-435b-aba2-1f1def84a5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
